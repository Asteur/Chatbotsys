{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on building configuration file for Keras classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is aimed to help users of the open-sourced library **DeepPavlov** to understand the structure of configuration files for **classification** models implemented in DeepPavlov on **Keras** (with tensorflow backend).\n",
    "\n",
    "Let's take a look at \"Detecting Insults in Social Commentary\" \n",
    "\n",
    "See here (https://www.kaggle.com/c/detecting-insults-in-social-commentary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firstly, please, download dataset, embedding file and pre-trained model for considered task via command in terminal:\n",
    "\n",
    "```\n",
    "python -m deeppavlov.deep download configs/sentiment/insults_kaggle.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary functions from DeepPavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from deeppavlov.core.common.file import read_json, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_json(data):\n",
    "    print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read one of the configs for classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = read_json(\"../../configs/sentiment/insults_kaggle.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dataset_reader\": {\n",
      "    \"name\": \"basic_classification_reader\",\n",
      "    \"x\": \"Comment\",\n",
      "    \"y\": \"Class\",\n",
      "    \"data_path\": \"insults_data\"\n",
      "  },\n",
      "  \"dataset_iterator\": {\n",
      "    \"name\": \"basic_classification_iterator\",\n",
      "    \"seed\": 42\n",
      "  },\n",
      "  \"chainer\": {\n",
      "    \"in\": [\n",
      "      \"x\"\n",
      "    ],\n",
      "    \"in_y\": [\n",
      "      \"y\"\n",
      "    ],\n",
      "    \"pipe\": [\n",
      "      {\n",
      "        \"id\": \"classes_vocab\",\n",
      "        \"name\": \"default_vocab\",\n",
      "        \"fit_on\": [\n",
      "          \"y\"\n",
      "        ],\n",
      "        \"level\": \"token\",\n",
      "        \"save_path\": \"vocabs/insults_kaggle_classes.dict\",\n",
      "        \"load_path\": \"vocabs/insults_kaggle_classes.dict\"\n",
      "      },\n",
      "      {\n",
      "        \"in\": [\n",
      "          \"x\"\n",
      "        ],\n",
      "        \"out\": [\n",
      "          \"x_prep\"\n",
      "        ],\n",
      "        \"name\": \"dirty_comments_preprocessor\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"my_embedder\",\n",
      "        \"name\": \"fasttext\",\n",
      "        \"save_path\": \"embeddings/wordpunct_tok_reddit_comments_2017_11_300.bin\",\n",
      "        \"load_path\": \"embeddings/wordpunct_tok_reddit_comments_2017_11_300.bin\",\n",
      "        \"dim\": 300\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"my_tokenizer\",\n",
      "        \"name\": \"nltk_tokenizer\",\n",
      "        \"tokenizer\": \"wordpunct_tokenize\"\n",
      "      },\n",
      "      {\n",
      "        \"in\": [\n",
      "          \"x_prep\"\n",
      "        ],\n",
      "        \"in_y\": [\n",
      "          \"y\"\n",
      "        ],\n",
      "        \"out\": [\n",
      "          \"y_labels\",\n",
      "          \"y_probas_dict\"\n",
      "        ],\n",
      "        \"main\": true,\n",
      "        \"name\": \"intent_model\",\n",
      "        \"save_path\": \"sentiment/insults_kaggle_v0\",\n",
      "        \"load_path\": \"sentiment/insults_kaggle_v0\",\n",
      "        \"classes\": \"#classes_vocab.keys()\",\n",
      "        \"kernel_sizes_cnn\": [\n",
      "          1,\n",
      "          2,\n",
      "          3\n",
      "        ],\n",
      "        \"filters_cnn\": 256,\n",
      "        \"confident_threshold\": 0.5,\n",
      "        \"optimizer\": \"Adam\",\n",
      "        \"lear_rate\": 0.01,\n",
      "        \"lear_rate_decay\": 0.1,\n",
      "        \"loss\": \"binary_crossentropy\",\n",
      "        \"last_layer_activation\": \"softmax\",\n",
      "        \"text_size\": 100,\n",
      "        \"coef_reg_cnn\": 0.001,\n",
      "        \"coef_reg_den\": 0.01,\n",
      "        \"dropout_rate\": 0.5,\n",
      "        \"dense_size\": 100,\n",
      "        \"model_name\": \"cnn_model\",\n",
      "        \"embedder\": \"#my_embedder\",\n",
      "        \"tokenizer\": \"#my_tokenizer\"\n",
      "      }\n",
      "    ],\n",
      "    \"out\": [\n",
      "      \"y_labels\",\n",
      "      \"y_probas_dict\"\n",
      "    ]\n",
      "  },\n",
      "  \"train\": {\n",
      "    \"epochs\": 1000,\n",
      "    \"batch_size\": 64,\n",
      "    \"metrics\": [\n",
      "      \"classification_accuracy\",\n",
      "      \"classification_f1\",\n",
      "      \"classification_roc_auc\"\n",
      "    ],\n",
      "    \"validation_patience\": 5,\n",
      "    \"val_every_n_epochs\": 5,\n",
      "    \"log_every_n_epochs\": 5,\n",
      "    \"show_examples\": false,\n",
      "    \"validate_best\": true,\n",
      "    \"test_best\": true\n",
      "  },\n",
      "  \"metadata\": {\n",
      "    \"labels\": {\n",
      "      \"telegram_utils\": \"IntentModel\",\n",
      "      \"server_utils\": \"KerasIntentModel\"\n",
      "    },\n",
      "    \"download\": [\n",
      "      \"http://lnsigo.mipt.ru/export/deeppavlov_data/vocabs.tar.gz\",\n",
      "      \"http://lnsigo.mipt.ru/export/deeppavlov_data/sentiment.tar.gz\",\n",
      "      \"http://lnsigo.mipt.ru/export/datasets/insults_data.tar.gz\",\n",
      "      {\n",
      "        \"url\": \"http://lnsigo.mipt.ru/export/embeddings/reddit_fastText/wordpunct_tok_reddit_comments_2017_11_300.bin\",\n",
      "        \"subdir\": \"embeddings\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset_reader\n",
    "\n",
    "DatasetReader parameters are determined by `config[\"dataset_reader\"]` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"basic_classification_reader\",\n",
      "  \"x\": \"Comment\",\n",
      "  \"y\": \"Class\",\n",
      "  \"data_path\": \"insults_data\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json(config[\"dataset_reader\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter `name` is a registered name of one of the DatasetReaders from DeepPavlov. \n",
    "\n",
    "One can either prepare data in particular format and use ready DatasetReader OR code and registere his/her own DatasetReader for dataset of interest.\n",
    "\n",
    "For example, the considered dataset was converted to the following format and now can be used with `basic_classification_reader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "      <td>Insult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "      <td>Not Insult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "      <td>Not Insult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "      <td>Not Insult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "      <td>Not Insult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment       Class\n",
       "0                               \"You fuck your dad.\"      Insult\n",
       "1  \"i really don't understand your point.\\xa0 It ...  Not Insult\n",
       "2  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...  Not Insult\n",
       "3  \"listen if you dont wanna get married to a man...  Not Insult\n",
       "4  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...  Not Insult"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"../../../download/insults_data/train.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for `basic_classification_reader`:**\n",
    "\n",
    "* The goal of the DatasetReader is to read data from the given `data_path` folder.\n",
    "\n",
    "* One can provide `train`, `valid` and/or `test` items to determine filenames for train, validation and test sets (by default `train.csv`, `valid.csv` and `test.csv`).\n",
    "\n",
    "* Datsets should be provided in `.csv` format. Additional parameters `sep`, `header`, `names` for `pandas.read_csv` can also be specified.\n",
    "\n",
    "* Items `x` and `y` determine column names (by default `text` and `labels`).\n",
    "\n",
    "* If dataset contains multi-labeled data, particular sample labels should be given in one column `y` separated by `class_sep` (by default `,`).\n",
    "\n",
    "Most parameters for the considered dataset are used by default but let us specify them below to just make clear how this part of config can look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config[\"dataset_reader\"][\"train\"] = \"train.csv\"\n",
    "config[\"dataset_reader\"][\"valid\"] = \"valid.csv\"\n",
    "config[\"dataset_reader\"][\"test\"] = \"test.csv\"\n",
    "config[\"dataset_reader\"][\"sep\"] = \",\"\n",
    "config[\"dataset_reader\"][\"class_sep\"] = \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"basic_classification_reader\",\n",
      "  \"x\": \"Comment\",\n",
      "  \"y\": \"Class\",\n",
      "  \"data_path\": \"insults_data\",\n",
      "  \"train\": \"train.csv\",\n",
      "  \"valid\": \"valid.csv\",\n",
      "  \"test\": \"test.csv\",\n",
      "  \"sep\": \",\",\n",
      "  \"class_sep\": \",\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json(config[\"dataset_reader\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset_iterator\n",
    "\n",
    "DatasetIterator is aimed to get data dictionary from DatasetReader and iterate over it batch-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"basic_classification_iterator\",\n",
      "  \"seed\": 42\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json(config[\"dataset_iterator\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter `name` is a registered name of one of the DatasetIterators from DeepPavlov. \n",
    "\n",
    "**Parameters for `basic_classification_iterator`:**\n",
    "\n",
    "* Boolean parameter `shuffle` (by default True) and integer parameter `seed`  determine whether to shuffle train data and which seed to use.\n",
    "\n",
    "* `basic_classification_iterator` allows to merge and/or split given data (for example, when validation set is not defined, or too big and smaller validation set can be separated). \n",
    "* `fields_to_merge` is a list of fields to be merged in one field `merged_field` (each field name is out of `train`, `valid`, `test`)\n",
    "* `field_to_split` is a name of field to be splitted in fields named by a list `split_fields` with `split_proportions` (list with values from 0 to 1).\n",
    "\n",
    "Most parameters for the considered dataset are used by default but let us specify them below to just make clear how this part of config can look like.\n",
    "\n",
    "An example below shows parameters if one wants to merge samples from train and validation files and then to separate one tenth to be validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config[\"dataset_iterator\"][\"shuffle\"] = True\n",
    "config[\"dataset_iterator\"][\"seed\"] = 42\n",
    "config[\"dataset_iterator\"][\"fields_to_merge\"] = [\"train\", \"valid\"]\n",
    "config[\"dataset_iterator\"][\"merged_field\"] = \"train\"\n",
    "config[\"dataset_iterator\"][\"field_to_split\"] = \"train\"\n",
    "config[\"dataset_iterator\"][\"split_proportions\"] = [0.9, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"basic_classification_iterator\",\n",
      "  \"seed\": 42,\n",
      "  \"shuffle\": true,\n",
      "  \"fields_to_merge\": [\n",
      "    \"train\",\n",
      "    \"valid\"\n",
      "  ],\n",
      "  \"merged_field\": \"train\",\n",
      "  \"field_to_split\": \"train\",\n",
      "  \"split_proportions\": [\n",
      "    0.9,\n",
      "    0.1\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json(config[\"dataset_iterator\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chainer\n",
    "\n",
    "Chainer is the biggest part of the config that determines structure and parameters for model pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"in\": [\n",
      "    \"x\"\n",
      "  ],\n",
      "  \"in_y\": [\n",
      "    \"y\"\n",
      "  ],\n",
      "  \"pipe\": [\n",
      "    {\n",
      "      \"id\": \"classes_vocab\",\n",
      "      \"name\": \"default_vocab\",\n",
      "      \"fit_on\": [\n",
      "        \"y\"\n",
      "      ],\n",
      "      \"level\": \"token\",\n",
      "      \"save_path\": \"vocabs/insults_kaggle_classes.dict\",\n",
      "      \"load_path\": \"vocabs/insults_kaggle_classes.dict\"\n",
      "    },\n",
      "    {\n",
      "      \"in\": [\n",
      "        \"x\"\n",
      "      ],\n",
      "      \"out\": [\n",
      "        \"x_prep\"\n",
      "      ],\n",
      "      \"name\": \"dirty_comments_preprocessor\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"my_embedder\",\n",
      "      \"name\": \"fasttext\",\n",
      "      \"save_path\": \"embeddings/wordpunct_tok_reddit_comments_2017_11_300.bin\",\n",
      "      \"load_path\": \"embeddings/wordpunct_tok_reddit_comments_2017_11_300.bin\",\n",
      "      \"dim\": 300\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"my_tokenizer\",\n",
      "      \"name\": \"nltk_tokenizer\",\n",
      "      \"tokenizer\": \"wordpunct_tokenize\"\n",
      "    },\n",
      "    {\n",
      "      \"in\": [\n",
      "        \"x_prep\"\n",
      "      ],\n",
      "      \"in_y\": [\n",
      "        \"y\"\n",
      "      ],\n",
      "      \"out\": [\n",
      "        \"y_labels\",\n",
      "        \"y_probas_dict\"\n",
      "      ],\n",
      "      \"main\": true,\n",
      "      \"name\": \"intent_model\",\n",
      "      \"save_path\": \"sentiment/insults_kaggle_v0\",\n",
      "      \"load_path\": \"sentiment/insults_kaggle_v0\",\n",
      "      \"classes\": \"#classes_vocab.keys()\",\n",
      "      \"kernel_sizes_cnn\": [\n",
      "        1,\n",
      "        2,\n",
      "        3\n",
      "      ],\n",
      "      \"filters_cnn\": 256,\n",
      "      \"confident_threshold\": 0.5,\n",
      "      \"optimizer\": \"Adam\",\n",
      "      \"lear_rate\": 0.01,\n",
      "      \"lear_rate_decay\": 0.1,\n",
      "      \"loss\": \"binary_crossentropy\",\n",
      "      \"last_layer_activation\": \"softmax\",\n",
      "      \"text_size\": 100,\n",
      "      \"coef_reg_cnn\": 0.001,\n",
      "      \"coef_reg_den\": 0.01,\n",
      "      \"dropout_rate\": 0.5,\n",
      "      \"dense_size\": 100,\n",
      "      \"model_name\": \"cnn_model\",\n",
      "      \"embedder\": \"#my_embedder\",\n",
      "      \"tokenizer\": \"#my_tokenizer\"\n",
      "    }\n",
      "  ],\n",
      "  \"out\": [\n",
      "    \"y_labels\",\n",
      "    \"y_probas_dict\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json(config[\"chainer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`chainer` has four main parameters `in`, `in_y`, `out` and `pipe`:\n",
    "\n",
    "* `in`, `in_y` and `out` denote names and structure of data transferred in pipeline. DatasetIterator `basic_dataset_iterator` provides data sample as tuple of two elements `(x, y)`: text and its labels.\n",
    "\n",
    "* `pipe` is a list of pipeline elements: vocabularies, preprocessors, embedders, tokenizers, model itself.\n",
    "\n",
    "* Every element in pipe should have specified `name` that is a registered name in DeepPavlov.\n",
    "\n",
    "* For further usage parameter `id` can be specified. For example, tokenizer should be given to `KerasModel` during initialization of model. Therefore, one should place a tokenizer element before model, specify `\"id\": \"my_tokenizer\"` and then refer to it `\"tokenizer\": \"#my_tokenizer\"` in model parameters.\n",
    "\n",
    "* If element of pipe processes data, `in` and `out` determine the order of data flow.\n",
    "\n",
    "* Other parameters for each element of pipe are individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab\n",
    "\n",
    "Considered classification model implies only one vocabulary that is used to extract all presented in the train set classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"classes_vocab\",\n",
      "  \"name\": \"default_vocab\",\n",
      "  \"fit_on\": [\n",
      "    \"y\"\n",
      "  ],\n",
      "  \"level\": \"token\",\n",
      "  \"save_path\": \"vocabs/insults_kaggle_classes.dict\",\n",
      "  \"load_path\": \"vocabs/insults_kaggle_classes.dict\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json(config[\"chainer\"][\"pipe\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `id` is a user-denoted name for further references in config\n",
    "* `default_vocab` is registered name of vocabulary\n",
    "* vocab is fitted on `y` (which denotes labels in the considered task) on token `level`\n",
    "* `save_path` and `load_path` denote where to load pre-trained vocabulary of labels from or where to save trained vocabulary of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessors\n",
    "\n",
    "One can use preprocessors presented in DeepPavlov or create his/her own preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"in\": [\n",
      "    \"x\"\n",
      "  ],\n",
      "  \"out\": [\n",
      "    \"x_prep\"\n",
      "  ],\n",
      "  \"name\": \"dirty_comments_preprocessor\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json(config[\"chainer\"][\"pipe\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor is a part of pipe that processes given data that means one has to define input and output names and structure. \n",
    "* `name` is a registered name of preprocessor from DeepPavlov\n",
    "* Considered preprocessor takes texts from `in` field of pipe, processes them to `out`. In this case preprocessor acts on `x` that is exact input of chainer (first element of tuple gengerated by DatasetIterator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedders and tokenizers\n",
    "\n",
    "Embedders and tokenizers are not exact elements of pipeline because they are not process data in pipeline. \n",
    "\n",
    "But embedder and tokenizer should be initialized before transferring them to the main model. \n",
    "Therefore, embedder and tokenizer should be placed somewhere before the main model in pipeline config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"my_embedder\",\n",
      "  \"name\": \"fasttext\",\n",
      "  \"save_path\": \"embeddings/wordpunct_tok_reddit_comments_2017_11_300.bin\",\n",
      "  \"load_path\": \"embeddings/wordpunct_tok_reddit_comments_2017_11_300.bin\",\n",
      "  \"dim\": 300\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json(config[\"chainer\"][\"pipe\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"my_tokenizer\",\n",
      "  \"name\": \"nltk_tokenizer\",\n",
      "  \"tokenizer\": \"wordpunct_tokenize\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json(config[\"chainer\"][\"pipe\"][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `id` is a user-denoted name for further references in config.\n",
    "* `name` is a registered name of embedder/tokenizer in DeepPavlov.\n",
    "* `save_path` and `load_path` denote  where to load pre-trained embedder/tokenizer from or where to save trained embedder/tokenizer.\n",
    "* Other additional parameters are accepted. For example, `dim` defines dimensionality of embedding model trained via fastText, and `tokenizer` defines tokenization mode for NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The main part of the classification pipeline is keras neural model itself.\n",
    "\n",
    "One can use either implemented in `KerasIntentModel` neural networks or implement his/her own network as a method of `KerasIntentModel` class.\n",
    "\n",
    "**Description of parameters for currently available neural networks is given at the end of this tutorial.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"in\": [\n",
      "    \"x_prep\"\n",
      "  ],\n",
      "  \"in_y\": [\n",
      "    \"y\"\n",
      "  ],\n",
      "  \"out\": [\n",
      "    \"y_labels\",\n",
      "    \"y_probas_dict\"\n",
      "  ],\n",
      "  \"main\": true,\n",
      "  \"name\": \"intent_model\",\n",
      "  \"save_path\": \"sentiment/insults_kaggle_v0\",\n",
      "  \"load_path\": \"sentiment/insults_kaggle_v0\",\n",
      "  \"classes\": \"#classes_vocab.keys()\",\n",
      "  \"kernel_sizes_cnn\": [\n",
      "    1,\n",
      "    2,\n",
      "    3\n",
      "  ],\n",
      "  \"filters_cnn\": 256,\n",
      "  \"confident_threshold\": 0.5,\n",
      "  \"optimizer\": \"Adam\",\n",
      "  \"lear_rate\": 0.01,\n",
      "  \"lear_rate_decay\": 0.1,\n",
      "  \"loss\": \"binary_crossentropy\",\n",
      "  \"last_layer_activation\": \"softmax\",\n",
      "  \"text_size\": 100,\n",
      "  \"coef_reg_cnn\": 0.001,\n",
      "  \"coef_reg_den\": 0.01,\n",
      "  \"dropout_rate\": 0.5,\n",
      "  \"dense_size\": 100,\n",
      "  \"model_name\": \"cnn_model\",\n",
      "  \"embedder\": \"#my_embedder\",\n",
      "  \"tokenizer\": \"#my_tokenizer\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json(config[\"chainer\"][\"pipe\"][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `in`, `in_y` and `out` denote names and structure of data transferred in pipeline. DatasetIterator `basic_dataset_iterator` provides data sample as tuple of two elements (`x`, `y`): text and its labels. Then preprocessor processes `x` to `x_prep`, and exactly this `x_prep` is an input for the main model along with `y` labels. For each sample the main model provides tuple of two elements (`y_labels`, `y_probas_dict`) where `y_labels` is an array of predicted classes (which sample belongs with), `y_probas_dict` is a dictionary like {\"class_i\": probability_i}.\n",
    "* `name` is a registered name of model in DeepPavlov.\n",
    "* `save_path` and `load_path` denote where to load pre-trained model from or where to save trained model.\n",
    "* `classes` contains names of all the presented in the train dataset classes. In the considered case it is presented as a reference to method `keys()` applied to the vocabulary of labels (`id` is used to refer).\n",
    "* `model_name` is a method name of `KerasIntentModel` class. **Currently available methods** are `cnn_model`, `dcnn_model`, `cnn_model_max_and_aver_pool`, `bilstm_model`, `bilstm_bilstm_model`,  `bilstm_cnn_model`, `cnn_bilstm_model`, `bilstm_self_add_attention_model`, `bilstm_self_mult_attention_model`, `bigru_model`.\n",
    "* `kernel_sizes_cnn`, `filters_cnn`, `dense_size`, `last_layer_activation`, `coef_reg_cnn`, `coef_reg_den`, `dropout_rate` are specific parameters for `cnn_model` method of `KerasIntentModel`.\n",
    "* `confident_threshold` is a boundary value of probability for converting probabilities to labels. The value is from 0 to 1. If all probabilities are lower than `confident_threshold`, label with the highest probability is assigned.\n",
    "* `optimizer` is a function from `keras.optimizers`.\n",
    "* `lear_rate`, `lear_rate_decay` is a learning rate and learning rate decay.\n",
    "* `loss` is a function from `keras.losses`.\n",
    "* `text_size` determines maximal length of text in tokens (words), longer texts are cutted, shorter ones are padded by zeros (pre-padding).\n",
    "* `embedder` and `tokenizer` are given by references to pipeline elements via theis `id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train parameters\n",
    "\n",
    "Another essential part of config files is a dictionary with train parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"epochs\": 1000,\n",
      "  \"batch_size\": 64,\n",
      "  \"metrics\": [\n",
      "    \"classification_accuracy\",\n",
      "    \"classification_f1\",\n",
      "    \"classification_roc_auc\"\n",
      "  ],\n",
      "  \"validation_patience\": 5,\n",
      "  \"val_every_n_epochs\": 5,\n",
      "  \"log_every_n_epochs\": 5,\n",
      "  \"show_examples\": false,\n",
      "  \"validate_best\": true,\n",
      "  \"test_best\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json(config[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `epochs` is a number of considered epochs.\n",
    "* `batch_size` is used for training and evaluation.\n",
    "* `metrics` is a list of names of registered metrics. For the examined task `classification_accuracy`, `classification_f1`, `classification_roc_auc` can be used because a special output (tuple of two elements) is considered.\n",
    "* `metric_optimization` determines whether to minimize or maximize the main metric (\"minimize\", \"maximize\"), by default `maximize`.\n",
    "* `validation_patience` is aparameter of early stopping: for how many epochs the training can continue without improvement of metric value on the validation set.\n",
    "* `val_every_n_epochs` is a frequency of validation during training (validate every n epochs).\n",
    "* `val_every_n_batches` is a frequency of validation during training (validate every n batches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "Additional information about model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"labels\": {\n",
      "    \"telegram_utils\": \"IntentModel\",\n",
      "    \"server_utils\": \"KerasIntentModel\"\n",
      "  },\n",
      "  \"download\": [\n",
      "    \"http://lnsigo.mipt.ru/export/deeppavlov_data/vocabs.tar.gz\",\n",
      "    \"http://lnsigo.mipt.ru/export/deeppavlov_data/sentiment.tar.gz\",\n",
      "    \"http://lnsigo.mipt.ru/export/datasets/insults_data.tar.gz\",\n",
      "    {\n",
      "      \"url\": \"http://lnsigo.mipt.ru/export/embeddings/reddit_fastText/wordpunct_tok_reddit_comments_2017_11_300.bin\",\n",
      "      \"subdir\": \"embeddings\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json(config[\"metadata\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `labels` determine labels or tags to make reference to this model.\n",
    "* `download`contains links for downloading all the components required for the considered model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn_model\n",
    "\n",
    "Shallow and wide convolutional NN.\n",
    "\n",
    "* `kernel_sizes_cnn` - list of kernel sizes of convolutions\n",
    "* `filters_cnn` - number of filters for convolutions\n",
    "* `coef_reg_cnn` - L2-regularization coefficient for convolutions\n",
    "* `dropout_rate` - dropout rate to be used after convolutions and between dense layers\n",
    "* `dense_size` - number of units for dense layer\n",
    "* `coef_reg_dense` - L2-regularization coefficient for dense layers\n",
    "* `last_layer_activation` - activation type for the last classification layer (by default `sigmoid`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dcnn_model\n",
    "\n",
    "Deep convolutional NN.\n",
    "\n",
    "* `kernel_sizes_cnn` - list of kernel sizes of convolutions\n",
    "* `filters_cnn` - list of numbers of filters for convolutions\n",
    "* `coef_reg_cnn` - L2-regularization coefficient for convolutions\n",
    "* `dropout_rate` - dropout rate to be used after convolutions and between dense layers\n",
    "* `dense_size` - number of units for dense layer\n",
    "* `coef_reg_dense` - L2-regularization coefficient for dense layers\n",
    "* `last_layer_activation` - activation type for the last classification layer (by default `sigmoid`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn_model_max_and_aver_pool\n",
    "\n",
    "Shallow and wide convolutional NN with concatentation of max and average pooling after convolutions.\n",
    "\n",
    "* `kernel_sizes_cnn` - list of kernel sizes of convolutions\n",
    "* `filters_cnn` - number of filters for convolutions\n",
    "* `coef_reg_cnn` - L2-regularization coefficient for convolutions\n",
    "* `dropout_rate` - dropout rate to be used after convolutions and between dense layers\n",
    "* `dense_size` - number of units for dense layer\n",
    "* `coef_reg_dense` - L2-regularization coefficient for dense layers\n",
    "* `last_layer_activation` - activation type for the last classification layer (by default `sigmoid`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bilstm_model\n",
    "\n",
    "Bidirectional Long short-term memory NN.\n",
    "\n",
    "* `units_lstm` - number of units for LSTM\n",
    "* `coef_reg_lstm` - L2-regularization coefficient for LSTM\n",
    "* `rec_dropout_rate` - droupout rate for LSTM\n",
    "* `dropout_rate` - dropout rate to be used after BiLSTM and between dense layers\n",
    "* `dense_size` - number of units for dense layer\n",
    "* `coef_reg_dense` - L2-regularization coefficient for dense layers\n",
    "* `last_layer_activation` - activation type for the last classification layer (by default `sigmoid`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bilstm_bilstm_model\n",
    "\n",
    "Two-layers bidirectional Long short-term memory NN.\n",
    "\n",
    "* `units_lstm_1` - number of units for the first LSTM layer\n",
    "* `units_lstm_2` - number of units for the second LSTM layer\n",
    "* `coef_reg_lstm` - L2-regularization coefficient for LSTM\n",
    "* `rec_dropout_rate` - droupout rate for LSTM\n",
    "* `dropout_rate` - dropout rate to be used between all BiLSTM and dense layers\n",
    "* `dense_size` - number of units for dense layer\n",
    "* `coef_reg_dense` - L2-regularization coefficient for dense layers\n",
    "* `last_layer_activation` - activation type for the last classification layer (by default `sigmoid`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bilstm_cnn_model\n",
    "\n",
    "Bidirectional Long short-term memory NN followed by shallow and wide Convolutional NN.\n",
    "\n",
    "* `units_lstm` - number of units for the first LSTM layer\n",
    "* `coef_reg_lstm` - L2-regularization coefficient for LSTM\n",
    "* `rec_dropout_rate` - droupout rate for LSTM\n",
    "* `kernel_sizes_cnn` - list of kernel sizes of convolutions\n",
    "* `filters_cnn` - number of filters for convolutions\n",
    "* `coef_reg_cnn` - L2-regularization coefficient for convolutions\n",
    "* `dropout_rate` - dropout rate to be used between BiLSTM and CNN, after CNN and between dense layers\n",
    "* `dense_size` - number of units for dense layer\n",
    "* `coef_reg_dense` - L2-regularization coefficient for dense layers\n",
    "* `last_layer_activation` - activation type for the last classification layer (by default `sigmoid`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn_bilstm_model\n",
    "\n",
    "Shallow-and-wide Convolutional NN followed by Bidirectional Long short-term memory NN.\n",
    "\n",
    "* `kernel_sizes_cnn` - list of kernel sizes of convolutions\n",
    "* `filters_cnn` - number of filters for convolutions\n",
    "* `coef_reg_cnn` - L2-regularization coefficient for convolutions\n",
    "* `units_lstm` - number of units for the first LSTM layer\n",
    "* `coef_reg_lstm` - L2-regularization coefficient for LSTM\n",
    "* `rec_dropout_rate` - droupout rate for LSTM\n",
    "* `dropout_rate` - dropout rate to be used between BiLSTM and CNN, after BiLSTM and between dense layers\n",
    "* `dense_size` - number of units for dense layer\n",
    "* `coef_reg_dense` - L2-regularization coefficient for dense layers\n",
    "* `last_layer_activation` - activation type for the last classification layer (by default `sigmoid`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bilstm_self_add_attention_model\n",
    "\n",
    "Bidirectional Long short-term memory NN with additive self-attention.\n",
    "\n",
    "* `units_lstm` - number of units for the first LSTM layer\n",
    "* `coef_reg_lstm` - L2-regularization coefficient for LSTM\n",
    "* `rec_dropout_rate` - droupout rate for LSTM\n",
    "* `self_att_hid` - number of hidden units for additive self-attention layer\n",
    "* `self_att_out` - number of output units for additive self-attention layer\n",
    "* `dropout_rate` - dropout rate to be used after self-attention layer and between dense layers\n",
    "* `dense_size` - number of units for dense layer\n",
    "* `coef_reg_dense` - L2-regularization coefficient for dense layers\n",
    "* `last_layer_activation` - activation type for the last classification layer (by default `sigmoid`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bilstm_self_mult_attention_model\n",
    "\n",
    "Bidirectional Long short-term memory NN with multiplicative self-attention.\n",
    "\n",
    "* `units_lstm` - number of units for the first LSTM layer\n",
    "* `coef_reg_lstm` - L2-regularization coefficient for LSTM\n",
    "* `rec_dropout_rate` - droupout rate for LSTM\n",
    "* `self_att_hid` - number of hidden units for multiplicative self-attention layer\n",
    "* `self_att_out` - number of output units for multiplicative self-attention layer\n",
    "* `dropout_rate` - dropout rate to be used after self-attention layer and between dense layers\n",
    "* `dense_size` - number of units for dense layer\n",
    "* `coef_reg_dense` - L2-regularization coefficient for dense layers\n",
    "* `last_layer_activation` - activation type for the last classification layer (by default `sigmoid`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigru_model\n",
    "\n",
    "Bidirectional Gated Recurrent Units NN.\n",
    "\n",
    "* `units_lstm` - number of units for the first GRU layer\n",
    "* `coef_reg_lstm` - L2-regularization coefficient for GRU\n",
    "* `rec_dropout_rate` - droupout rate for GRU\n",
    "* `dropout_rate` - dropout rate to be used after BiGRU| and between dense layers\n",
    "* `dense_size` - number of units for dense layer\n",
    "* `coef_reg_dense` - L2-regularization coefficient for dense layers\n",
    "* `last_layer_activation` - activation type for the last classification layer (by default `sigmoid`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-deep36",
   "language": "python",
   "name": "deep36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
